{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing avant utilisation des modèles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook vise à explorer rapidement la base de données pour avoir une idée de se composition, la répartition et éventuels déséquilibres entre les classes, les caractéristiques des textes à analyser. Cela est éventuellement l'occasion de détecter des erreurs de textes (encodage erroné au moment d'enregistrer/lire les demandes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Pour faciliter la mise à jour des fonctions écrites dans func_custom sans avoir à redémarrer le kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages classiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom package\n",
    "import func_custom as fc\n",
    "\n",
    "# NLP\n",
    "import unidecode\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/data_train.xlsx\",\n",
    "                    usecols = [\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyse, justification et étapes du preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si certaines étapes sont standards et ne nécessite pas d'explication (mettre en .lower() par exemple), regardons certains points plus particulièrement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Application de .lower() : non commenté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Encodage des accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prélèvement', 'époux', 'à', 'impôts']\n",
      "['prélèvement', 'époux', 'à', 'impôts']\n"
     ]
    }
   ],
   "source": [
    "first_message = df[\"message\"][0].split()\n",
    "data_encodage = [first_message[i] for i in [21, 37, 43, 46]]\n",
    "print(data_encodage)\n",
    "keyboard_encodage = [\"prélèvement\", \"époux\", \"à\", \"impôts\"]\n",
    "print(keyboard_encodage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodage des accents\n",
      "[b'pre\\\\u0301le\\\\u0300vement', b'e\\\\u0301poux', b'a\\\\u0300', b'impo\\\\u0302ts']\n",
      "[b'pr\\\\xe9l\\\\xe8vement', b'\\\\xe9poux', b'\\\\xe0', b'imp\\\\xf4ts']\n",
      "Conséquence sur la lemmatization\n",
      "['prélèv', 'époux', 'à', 'impôt']\n",
      "['prélev', 'époux', 'à', 'impôt']\n",
      "Conséquence sur l'embedding\n",
      "[0.0, 0.0, -37.616055, 0.0]\n",
      "[-0.74289227, 20.48502, -41.42349, 39.59558]\n",
      "Encodage après normalisation\n",
      "[b'pr\\\\xe9l\\\\xe8vement', b'\\\\xe9poux', b'\\\\xe0', b'imp\\\\xf4ts']\n",
      "[b'pr\\\\xe9l\\\\xe8vement', b'\\\\xe9poux', b'\\\\xe0', b'imp\\\\xf4ts']\n",
      "Embedding après normalisation\n",
      "[-0.74289227, 20.48502, -41.42349, 39.59558]\n",
      "[-0.74289227, 20.48502, -41.42349, 39.59558]\n"
     ]
    }
   ],
   "source": [
    "print(\"Encodage des accents\")\n",
    "print([t.encode(\"unicode_escape\") for t in data_encodage])\n",
    "print([t.encode(\"unicode_escape\") for t in keyboard_encodage])\n",
    "\n",
    "print(\"Conséquence sur la lemmatization\")\n",
    "stemmer = SnowballStemmer('french')\n",
    "print([stemmer.stem(t) for t in data_encodage])\n",
    "print([stemmer.stem(t) for t in keyboard_encodage])\n",
    "\n",
    "print(\"Conséquence sur l'embedding\")\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "print([np.sum(nlp(t)[0].vector) for t in data_encodage])\n",
    "print([np.sum(nlp(t)[0].vector) for t in keyboard_encodage])\n",
    "\n",
    "print(\"Encodage après normalisation\")\n",
    "print([unicodedata.normalize(\"NFKC\", t).encode(\"unicode_escape\") for t in data_encodage])\n",
    "print([unicodedata.normalize(\"NFKC\", t).encode(\"unicode_escape\") for t in keyboard_encodage])\n",
    "\n",
    "print(\"Embedding après normalisation\")\n",
    "print([np.sum(nlp(unicodedata.normalize(\"NFKC\", t))[0].vector) for t in data_encodage])\n",
    "print([np.sum(nlp(unicodedata.normalize(\"NFKC\", t))[0].vector) for t in keyboard_encodage])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faudra une étape de normalisation avec `unicodedata.normalize(\"NFKC\", text)` sinon l'embedding de Spacy utilisé par la suite retournement des vecteurs nuls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gestion de la ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les signes de ponctuations sont tous remplacés par un _espace_ pour anticiper sur les failles du tokenizer de nltk. Typiquement en cas d'erreur de phrase, d'absence d'espace, surtout autour de mots inconnus le tokenizer rate complètement. Exemple avec cette phrase qui contient `0000.L'XXXXX́e` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, J'ai divorcé en 0000.J'ai commencé à verser une pension alimentaire à partir de septembre 0000.L'XXXXX́e dernière j'ai donc versé un XXXXX́e pleine.Par rapport au prélèvement à la source comment cela ce passe-t-il sachant que j'aurais plus à déduire pour l'XXXXX́e dernière que 0000?actuellement je suis prélever par rapport à 0000?y-aura t-il un remboursement en fin d'XXXXX́e?Peut-on modifier le prélèvement à la source en cours d'XXXXX́e? D'avance merci Cordialement XXXXX XXXXX\n"
     ]
    }
   ],
   "source": [
    "message_test = df[df[\"message\"].str.contains(\"0000.L'XXXXX́e\")][\"message\"].values[0]\n",
    "print(message_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(message_test, language = \"french\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate un problème avec le tokenizer qui persisterait si on supprimait simplement la ponctuation, je préfère donc la remplacer par un espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pension',\n",
       " 'alimentaire',\n",
       " 'à',\n",
       " 'partir',\n",
       " 'de',\n",
       " 'septembre',\n",
       " \"0000.L'XXXXX́e\",\n",
       " 'dernière',\n",
       " \"j'ai\",\n",
       " 'donc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(fc.replace_punctuation_with_space(message_test), language = \"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partir',\n",
       " 'de',\n",
       " 'septembre',\n",
       " '0000',\n",
       " 'L',\n",
       " 'XXXXX́e',\n",
       " 'dernière',\n",
       " 'j',\n",
       " 'ai',\n",
       " 'donc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[15:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'utilise ici le tokenizer par défaut de nltk en français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Suppression des caractères de taille 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour prendre en compte des caractères spéciaux ou des lettres uniques suite au tokenizer, qui n'apporteront pas d'informations. Cela permet de limiter la taille des stopwords à inclure. Par ailleurs avec l'encodage diacritique la longueur de `à` serait en réalité de deux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Gestion des stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En combinant tous les éléments précédents dans la fonction `preprocess_stopwords` dans `func_custom.py` étudions désormais le traitement des stopwords. \n",
    "Commençons pas ne prendre en compte que ceux de base dans nltk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_french = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           \n",
       "xxxxx           1777\n",
       "taux            1323\n",
       "revenus          712\n",
       "prélèvement      670\n",
       "bonjour          624\n",
       "source           526\n",
       "cordialement     413\n",
       "plus             366\n",
       "merci            365\n",
       "déclaration      300\n",
       "comment          271\n",
       "faire            260\n",
       "bien             256\n",
       "situation        255\n",
       "xxxxx́e          253\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(df[\"message\"].dropna())\n",
    "tokens_clean = fc.preprocess_stopwords(text, stopwords_french)\n",
    "pd.DataFrame(tokens_clean).value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la simple inclusion des stopwords par défaut de nltk ne suffira pas, des éléments spécifiques au jeu de données (`xxxxx` et `0000` issus de pseudonymisation, `bonjour` et `merci` et mots de politesse du fait qu'il s'agit de message écrits par des particuliers à la DGFIP). On peut alors compléter de manière _ad hoc_ cette liste en regardant à l'oeil nu ces éléments. Si l'exerice consiste cependant à distinguer les messages \"polis\" des autres alors cette liste de stopwords n'est absolument pas pertinente.\n",
    "\n",
    "TF-IDF aurait dans une certaine mesure pu tenir compte de ces mots largement présent dans le corpus et peu informatif, autant traiter ce problème à la racine ce qui limitera la taille des données à traiter par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_adhoc = {\"à\", \"xxxxx\", \"bonjour\", \"cordialement\", \"merci\", \"xxxxx́e\", \"xxxxx́\", \"k€\", \"donc\", \"car\", \"cette\", \"cela\",\n",
    "                  \"être\", \"si\", \"même\", \"faire\", \"avoir\", \"remercie\", \"madame\", \"monsieur\"}\n",
    "stopwords_complete = stopwords_french.union(stopwords_adhoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            \n",
       "taux             1323\n",
       "revenus           712\n",
       "prélèvement       670\n",
       "source            526\n",
       "plus              366\n",
       "déclaration       300\n",
       "comment           271\n",
       "bien              256\n",
       "situation         255\n",
       "mois              249\n",
       "retraite          232\n",
       "depuis            229\n",
       "salaire           213\n",
       "impôt             210\n",
       "compte            201\n",
       "avance            201\n",
       "euros             192\n",
       "individualisé     183\n",
       "réponse           181\n",
       "impôts            180\n",
       "épouse            176\n",
       "revenu            174\n",
       "janvier           174\n",
       "montant           166\n",
       "pension           163\n",
       "fait              163\n",
       "conjoint          154\n",
       "dois              151\n",
       "imposition        145\n",
       "changement        145\n",
       "suite             141\n",
       "espace            134\n",
       "pouvez            128\n",
       "part              126\n",
       "emploi            126\n",
       "deux              112\n",
       "prélevé           111\n",
       "foyer             110\n",
       "alors             110\n",
       "déclarer          107\n",
       "payer             105\n",
       "mari              104\n",
       "non               100\n",
       "pacs               99\n",
       "concernant         99\n",
       "va                 98\n",
       "savoir             97\n",
       "demande            94\n",
       "modifier           91\n",
       "pourquoi           88\n",
       "jour               87\n",
       "mme                87\n",
       "question           85\n",
       "imposable          84\n",
       "comme              83\n",
       "possible           83\n",
       "actuellement       82\n",
       "entre              82\n",
       "personnalisé       82\n",
       "afin               79\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_clean = fc.preprocess_stopwords(text, stopwords_complete)\n",
    "pd.DataFrame(tokens_clean).value_counts().head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Prise en compte des fautes d'orthographe : une méthode simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ne vais pas essayer de prendre en compte exhaustivement des fautes d'orthographe mais me contenter d'identifier les erreurs les plus fréquentes dans le corpus. Cela me permettra 1) de corriger les erreurs fréquentes mais utile à garder (par exemple un accent manquant dans prélèvement) et 2) augmenter retrospectivement la liste des stopwords avec des abbréviations non pertinentes (`mme`, `mr`, `svp`).\n",
    "\n",
    "**Important , on constate la présence récurrente du PACS or on peut anticiper qu'il y aura un soucis de vocabulaire lors des méthodes d'embedding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/mots dictionnaires.txt\", \"r\", encoding= \"utf-8\") as fichier:\n",
    "   # Lire toutes les lignes du fichier et les stocker dans une liste\n",
    "   dic_french = set([(unicodedata.normalize(\"NFKC\", ligne.strip().lower())) for ligne in fichier.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_investiguer = []\n",
    "for elem in tokens_clean:\n",
    "    if not elem in dic_french:\n",
    "        typo_investiguer.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          \n",
       "pacs           99\n",
       "mme            87\n",
       "er             62\n",
       "impots         55\n",
       "pacsé          53\n",
       "xx             51\n",
       "mr             44\n",
       "pole           39\n",
       "pacsée         28\n",
       "svp            28\n",
       "impot          27\n",
       "pacsés         26\n",
       "aujourd        26\n",
       "gouv           24\n",
       "bnc            21\n",
       "n°             19\n",
       "cdd            18\n",
       "prelevement    17\n",
       "prélévement    17\n",
       "france         17\n",
       "carsat         16\n",
       "aout           16\n",
       "connaitre      14\n",
       "cdi            14\n",
       "etant          14\n",
       "fr             13\n",
       "agirc          12\n",
       "cdt            11\n",
       "eur            11\n",
       "xxxxx́s        11\n",
       "plait          10\n",
       "rib            10\n",
       "prélevement    10\n",
       "xxxxx́es        9\n",
       "ir              9\n",
       "ca              9\n",
       "meme            8\n",
       "xxxxxe          8\n",
       "arrco           8\n",
       "email           8\n",
       "chomage         8\n",
       "pls             7\n",
       "puisqu          7\n",
       "fip             7\n",
       "mlle            6\n",
       "reponse         6\n",
       "etre            6\n",
       "infos           6\n",
       "csg             6\n",
       "tns             6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(typo_investiguer).value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_adhoc = {\"à\", \"xxxxx\", \"bonjour\", \"cordialement\", \"merci\", \"xxxxx́e\", \"xxxxx́\", \"k€\", \"donc\", \"car\", \"cette\", \"cela\",\n",
    "                  \"être\", \"si\", \"même\", \"faire\", \"avoir\", \"remercie\", \"madame\", \"monsieur\",\n",
    "                  \"mme\", \"mr\", \"er\", \"xx\", \"svp\"}\n",
    "stopwords_complete = stopwords_french.union(stopwords_adhoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_list = {\n",
    "    \"impot\" : \"impôt\",\n",
    "    \"prélévement\" : \"prélèvement\",\n",
    "    \"prelevement\" : \"prélèvement\",\n",
    "    \"pole\" : \"pôle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "# tokens = [stemmer.stem(mot) for mot in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prélev'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"prélèvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prélev'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"prélevé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Agrégation dans une fonction unique et application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"message_clean\"] = df[\"message\"].apply(lambda x : fc.preprocess_text(x, stopwords_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, Je me permets de vous contacter afin d'obtenir une réponse sur ma déclaration de revenus. Nous avons effectué notre déclaration commune avec ma conjointe (nous ne l'avons pas encore validée), et on nous informe un taux pour le prélèvement à la source pour le foyer de 0000,0000% ou bien, si on le souhaite, un taux individualisé de 0000,0000% pour ma conjointe et de 0000,0000% pour le mien. Actuellement je disposais d'un taux individuel de 0000,0000% comment cela se fait t-il que mon taux individuel augmente de 0000,0000% alors que je n'ai pas déclaré plus que l'XXXXX́e passée ? Ci-dessous extrait du résumé de déclaration : \"Vous venez de signaler un changement de situation de famille au titre de l'XXXXX́e 0000. Cet événement impacte votre situation au regard du prélèvement à la source. En conséquence, le taux de prélèvement à la source qui sera transmis aux organismes vous versant des revenus est de : 0000,0000 % . Il est applicable immédiatement. Après avoir signé votre déclaration, vous pourrez opter, si vous le souhaitez, pour un taux individualisé, soit 0000,0000 % pour Madame XXXXX XXXXX et 0000,0000 % pour Monsieur XXXXX XXXXX XXXXX XXXXX.\" Dans l'attente de vous lire afin de pouvoir valider notre déclaration commune, Cordialement, Mr XXXXX XXXXX\n",
      "permets contacter afin obtenir réponse déclaration revenus effectué déclaration commune conjointe encore validée informe taux prélèvement source foyer bien souhaite taux individualisé conjointe mien actuellement disposais taux individuel comment fait taux individuel augmente alors déclaré plus passée ci dessous extrait résumé déclaration venez signaler changement situation famille titre cet événement impacte situation regard prélèvement source conséquence taux prélèvement source transmis organismes versant revenus applicable immédiatement après signé déclaration pourrez opter souhaitez taux individualisé attente lire afin pouvoir valider déclaration commune\n"
     ]
    }
   ],
   "source": [
    "message_test = df.sample(1)\n",
    "print(message_test[\"message\"].values[0])\n",
    "print(message_test[\"message_clean\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/data_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/data_clean.csv\", \n",
    "                sep = \";\",\n",
    "                index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
