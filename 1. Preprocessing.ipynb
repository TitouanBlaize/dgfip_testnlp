{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing avant utilisation des modèles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook détaille les étapes de preprocessing implémentées et leur justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour faciliter la mise à jour des fonctions écrites dans func_custom sans avoir à redémarrer le kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages classiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom package\n",
    "import func_custom as fc\n",
    "\n",
    "# NLP\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/data_train.xlsx\",\n",
    "                    usecols = [\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyse, justification et étapes du preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si certaines étapes sont standards et ne nécessite pas d'explication (mettre en .lower() par exemple), regardons certains points plus particulièrement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Application de .lower() : non commenté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Encodage des accents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a pu constater des problèmes dans le WordCloud du notebook précédent, regardons plus précisément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prélèvement', 'époux', 'à', 'impôts']\n",
      "['prélèvement', 'époux', 'à', 'impôts']\n"
     ]
    }
   ],
   "source": [
    "# En apparence les mots issus du .xlsx et écrits au clavier se ressemblent\n",
    "first_message = df[\"message\"][0].split()\n",
    "data_encodage = [first_message[i] for i in [21, 37, 43, 46]]\n",
    "print(data_encodage)\n",
    "keyboard_encodage = [\"prélèvement\", \"époux\", \"à\", \"impôts\"]\n",
    "print(keyboard_encodage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodage des accents\n",
      "[b'pre\\\\u0301le\\\\u0300vement', b'e\\\\u0301poux', b'a\\\\u0300', b'impo\\\\u0302ts']\n",
      "[b'pr\\\\xe9l\\\\xe8vement', b'\\\\xe9poux', b'\\\\xe0', b'imp\\\\xf4ts']\n",
      "\n",
      "\n",
      "Conséquence sur la lemmatization : pas les mêmes racines\n",
      "['prélèv', 'époux', 'à', 'impôt']\n",
      "['prélev', 'époux', 'à', 'impôt']\n",
      "\n",
      "\n",
      "Conséquence sur l'embedding : en dehors du dictionnaire pour spacy !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TitouanBlaize\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\TitouanBlaize\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, -37.616055, 0.0]\n",
      "[-0.74289227, 20.48502, -41.42349, 39.59558]\n",
      "\n",
      "\n",
      "Encodage après normalisation\n",
      "[b'pr\\\\xe9l\\\\xe8vement', b'\\\\xe9poux', b'\\\\xe0', b'imp\\\\xf4ts']\n",
      "[b'pr\\\\xe9l\\\\xe8vement', b'\\\\xe9poux', b'\\\\xe0', b'imp\\\\xf4ts']\n",
      "\n",
      "\n",
      "Embedding après normalisation : tout rentre dans l'ordre\n",
      "[-0.74289227, 20.48502, -41.42349, 39.59558]\n",
      "[-0.74289227, 20.48502, -41.42349, 39.59558]\n"
     ]
    }
   ],
   "source": [
    "print(\"Encodage des accents\")\n",
    "print([t.encode(\"unicode_escape\") for t in data_encodage])\n",
    "print([t.encode(\"unicode_escape\") for t in keyboard_encodage])\n",
    "print(\"\\n\")\n",
    "print(\"Conséquence sur la lemmatization : pas les mêmes racines\")\n",
    "stemmer = SnowballStemmer('french')\n",
    "print([stemmer.stem(t) for t in data_encodage])\n",
    "print([stemmer.stem(t) for t in keyboard_encodage])\n",
    "print(\"\\n\")\n",
    "print(\"Conséquence sur l'embedding : en dehors du dictionnaire pour spacy !\")\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "print([np.sum(nlp(t)[0].vector) for t in data_encodage])\n",
    "print([np.sum(nlp(t)[0].vector) for t in keyboard_encodage])\n",
    "print(\"\\n\")\n",
    "print(\"Encodage après normalisation\")\n",
    "print([unicodedata.normalize(\"NFKC\", t).encode(\"unicode_escape\") for t in data_encodage])\n",
    "print([unicodedata.normalize(\"NFKC\", t).encode(\"unicode_escape\") for t in keyboard_encodage])\n",
    "print(\"\\n\")\n",
    "print(\"Embedding après normalisation : tout rentre dans l'ordre\")\n",
    "print([np.sum(nlp(unicodedata.normalize(\"NFKC\", t))[0].vector) for t in data_encodage])\n",
    "print([np.sum(nlp(unicodedata.normalize(\"NFKC\", t))[0].vector) for t in keyboard_encodage])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faudra une étape de normalisation avec `unicodedata.normalize(\"NFKC\", text)` sinon l'embedding de Spacy utilisé par la suite retourne des vecteurs nuls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gestion de la ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les signes de ponctuations sont tous remplacés par un _espace_ pour anticiper sur les failles du tokenizer de nltk. Typiquement en cas d'erreur de phrase, d'absence d'espace, surtout autour de mots inconnus le tokenizer rate complètement, c'est normal il s'agit ici de textes libres écrits par des contribuables et non des textes académiques/de presse. Exemple avec cette phrase qui contient `0000.L'XXXXX́e` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, J'ai divorcé en 0000.J'ai commencé à verser une pension alimentaire à partir de septembre 0000.L'XXXXX́e dernière j'ai donc versé un XXXXX́e pleine.Par rapport au prélèvement à la source comment cela ce passe-t-il sachant que j'aurais plus à déduire pour l'XXXXX́e dernière que 0000?actuellement je suis prélever par rapport à 0000?y-aura t-il un remboursement en fin d'XXXXX́e?Peut-on modifier le prélèvement à la source en cours d'XXXXX́e? D'avance merci Cordialement XXXXX XXXXX\n"
     ]
    }
   ],
   "source": [
    "message_test = df[df[\"message\"].str.contains(\"0000.L'XXXXX́e\")][\"message\"].values[0]\n",
    "print(message_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(message_test, language = \"french\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate un problème avec le tokenizer qui persisterait si on supprimait simplement la ponctuation, je préfère donc la remplacer par un espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0000.L'XXXXX́e\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(fc.replace_punctuation_with_space(message_test), language = \"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000', 'L', 'XXXXX́e']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[18:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'utilise ici le tokenizer par défaut de nltk en français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Suppression des caractères de taille 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour prendre en compte des caractères spéciaux ou des lettres uniques suite au tokenizer, qui n'apporteront pas d'informations. Cela permet de limiter la taille des stopwords à inclure. Par ailleurs avec l'encodage diacritique la longueur de `à` serait en réalité de deux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Gestion des stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En combinant tous les éléments précédents dans la fonction `preprocess_stopwords` dans `func_custom.py` étudions désormais le traitement des stopwords. \n",
    "Commençons pas ne prendre en compte que ceux de base dans nltk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_french = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           \n",
       "xxxxx           1777\n",
       "taux            1323\n",
       "revenus          712\n",
       "prélèvement      670\n",
       "bonjour          624\n",
       "source           526\n",
       "cordialement     413\n",
       "plus             366\n",
       "merci            365\n",
       "déclaration      300\n",
       "comment          271\n",
       "faire            260\n",
       "bien             256\n",
       "situation        255\n",
       "xxxxx́e          253\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(df[\"message\"].dropna())\n",
    "tokens_clean = fc.preprocess_stopwords(text, stopwords_french)\n",
    "pd.DataFrame(tokens_clean).value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la simple inclusion des stopwords par défaut de nltk ne suffira pas, des éléments spécifiques au jeu de données (`xxxxx` et `0000` issus de pseudonymisation, `bonjour` et `merci` et mots de politesse du fait qu'il s'agit de message écrits par des particuliers à la DGFIP). On peut alors compléter de manière _ad hoc_ cette liste en regardant à l'oeil nu ces éléments. Si l'exercice consiste cependant à distinguer les messages \"polis\" des autres alors cette liste de stopwords n'est pas pertinente.\n",
    "\n",
    "TF-IDF aurait dans une certaine mesure pu tenir compte de ces mots largement présent dans le corpus et peu informatif, autant traiter ce problème à la racine ce qui limitera la taille des données à traiter par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_adhoc = {\"à\", \"xxxxx\", \"bonjour\", \"cordialement\", \"merci\", \"xxxxx́e\", \"xxxxx́\", \"k€\", \"donc\", \"car\", \"cette\", \"cela\",\n",
    "                  \"être\", \"si\", \"même\", \"faire\", \"avoir\", \"remercie\", \"madame\", \"monsieur\"}\n",
    "stopwords_complete = stopwords_french.union(stopwords_adhoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            \n",
       "taux             1323\n",
       "revenus           712\n",
       "prélèvement       670\n",
       "source            526\n",
       "plus              366\n",
       "déclaration       300\n",
       "comment           271\n",
       "bien              256\n",
       "situation         255\n",
       "mois              249\n",
       "retraite          232\n",
       "depuis            229\n",
       "salaire           213\n",
       "impôt             210\n",
       "compte            201\n",
       "avance            201\n",
       "euros             192\n",
       "individualisé     183\n",
       "réponse           181\n",
       "impôts            180\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_clean = fc.preprocess_stopwords(text, stopwords_complete)\n",
    "pd.DataFrame(tokens_clean).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Prise en compte des fautes d'orthographe : une méthode simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ne vais pas essayer de prendre en compte exhaustivement des fautes d'orthographe mais me contenter d'identifier les erreurs les plus fréquentes dans le corpus. Cela me permettra 1) de corriger les erreurs fréquentes mais utile à garder (par exemple un accent manquant dans prélèvement) et 2) augmenter retrospectivement la liste des stopwords avec des abbréviations non pertinentes (`mme`, `mr`, `svp`).\n",
    "\n",
    "Important , on constate la présence récurrente du PACS or on peut anticiper qu'il y aura un soucis de vocabulaire lors des méthodes d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargeons une liste de mots français, par exemple au lien suivant :\n",
    "#  https://github.com/chrplr/openlexicon/blob/master/datasets-info/Liste-de-mots-francais-Gutenberg/README-liste-francais-Gutenberg.md\n",
    "with open(\"data/mots dictionnaires.txt\", \"r\", encoding= \"utf-8\") as fichier:\n",
    "   # Lire toutes les lignes du fichier et les stocker dans une liste\n",
    "   dic_french = set([(unicodedata.normalize(\"NFKC\", ligne.strip().lower())) for ligne in fichier.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_investiguer = []\n",
    "for elem in tokens_clean:\n",
    "    if not elem in dic_french:\n",
    "        typo_investiguer.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          \n",
       "pacs           99\n",
       "mme            87\n",
       "er             62\n",
       "impots         55\n",
       "pacsé          53\n",
       "xx             51\n",
       "mr             44\n",
       "pole           39\n",
       "pacsée         28\n",
       "svp            28\n",
       "impot          27\n",
       "pacsés         26\n",
       "aujourd        26\n",
       "gouv           24\n",
       "bnc            21\n",
       "n°             19\n",
       "cdd            18\n",
       "prelevement    17\n",
       "prélévement    17\n",
       "france         17\n",
       "carsat         16\n",
       "aout           16\n",
       "connaitre      14\n",
       "cdi            14\n",
       "etant          14\n",
       "fr             13\n",
       "agirc          12\n",
       "cdt            11\n",
       "eur            11\n",
       "xxxxx́s        11\n",
       "plait          10\n",
       "rib            10\n",
       "prélevement    10\n",
       "xxxxx́es        9\n",
       "ir              9\n",
       "ca              9\n",
       "meme            8\n",
       "xxxxxe          8\n",
       "arrco           8\n",
       "email           8\n",
       "chomage         8\n",
       "pls             7\n",
       "puisqu          7\n",
       "fip             7\n",
       "mlle            6\n",
       "reponse         6\n",
       "etre            6\n",
       "infos           6\n",
       "csg             6\n",
       "tns             6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(typo_investiguer).value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_adhoc = {\"à\", \"xxxxx\", \"bonjour\", \"cordialement\", \"merci\", \"xxxxx́e\", \"xxxxx́\", \"k€\", \"donc\", \"car\", \"cette\", \"cela\",\n",
    "                  \"être\", \"si\", \"même\", \"faire\", \"avoir\", \"remercie\", \"madame\", \"monsieur\",\n",
    "                  \"mme\", \"mr\", \"er\", \"xx\", \"svp\"}\n",
    "stopwords_complete = stopwords_french.union(stopwords_adhoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_list = {\n",
    "    \"impot\" : \"impôt\",\n",
    "    \"prélévement\" : \"prélèvement\",\n",
    "    \"prelevement\" : \"prélèvement\",\n",
    "    \"pole\" : \"pôle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'utilisais un des stemmer classiques de NLTK, et qui est adapté au Français : SnowballStemmer. Cependant j'ai constaté une légère dégradation des performances du TF-IDF avec stemming, je ne l'utilise finalement pas pour le preprocess et préfère une lemmetizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "# tokens = [stemmer.stem(mot) for mot in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Lemmetizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu du stemming je peux avoir recours au lemmetizer, par exemple celui de Spacy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"fr_core_news_md\")\n",
    "# tokens = [token.lemma_ for token in nlp(\" \".join(tokens))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Agrégation dans une fonction unique et application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Très long à cause de l'ajout du lemmetizer de Spacy (35min au lieu de 1min avant)\n",
    "df[\"message_clean\"] = df[\"message\"].apply(lambda x : fc.preprocess_text(x, stopwords_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, Selon le taux individualisé qui m'est alloué pour le prélèvement à la source, je ne sais pas combien devra être m'être prélevé chaque mois. Le montant prélevé sera-t-il variable ou restera t-il constant? De plus, que faisons nous des factures relatives à la garde des enfants pour 0000 et 0000? Je vous remercie par avance de votre réponse.\n",
      "selon taux individualisé allouer prélèvement source savoir combien devoir prélever chaque mois montant prélever variable rester constant plus faire facture relatif garde enfant avancer réponse\n"
     ]
    }
   ],
   "source": [
    "message_test = df.sample(1)\n",
    "print(message_test[\"message\"].values[0])\n",
    "print(message_test[\"message_clean\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/data_clean.csv\", \n",
    "          sep = \";\",\n",
    "          index = False)\n",
    "\n",
    "# Autre possibilité de _serialization_\n",
    "# df.to_pickle(\"data/data_clean.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
